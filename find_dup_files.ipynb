{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891bd0ff",
   "metadata": {},
   "source": [
    "# 查找重复文件\n",
    "\n",
    "\n",
    "\n",
    "整理移动硬盘的时候，发现了有不少重复文件，所以想了想，是否可以用程序自动搜索重复文件，然后就可以有针对性地删除。\n",
    "\n",
    "https://blog.csdn.net/sinat_38682860/article/details/108387401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecad833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历文件夹 diskwalk.py\n",
    "\n",
    "import os, sys\n",
    "\n",
    "class diskwalk:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "    def paths(self):\n",
    "        path = self.path\n",
    "        # 迭代器，防止所有数据塞进内存爆掉\n",
    "        path_collection = (os.path.join(root, fn) for root, dirs, files in os.walk(path) for fn in files)\n",
    "        return path_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1c8f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 MD5值得代码 checksum.py\n",
    "\n",
    "import hashlib, sys\n",
    "\n",
    "# 分块读取 MD，速度较快\n",
    "def create_checksum(path):\n",
    "    # 以二进制方式读取文件，避免各种编码问题\n",
    "    fp = open(path, 'rb')\n",
    "    checksum = hashlib.md5()\n",
    "    while True:\n",
    "        buffer = fp.read(8192)\n",
    "        if not buffer:\n",
    "            break\n",
    "        checksum.update(buffer)\n",
    "    fp.close()\n",
    "    checksum = checksum.digest()\n",
    "    return checksum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f23e19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import getsize\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766edb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主函数代码\n",
    "\n",
    "from checksum import create_checksum\n",
    "from diskwalk import diskwalk\n",
    "from os.path import getsize\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "def findDupes(path):\n",
    "    record = {}\n",
    "    dup = {}\n",
    "    d = diskwalk(path)\n",
    "    files = d.paths()\n",
    "    for file in files:\n",
    "        #compound_key = (getsize(file), create_checksum(file))\n",
    "        compound_key = (getsize(file), file.split(\"\\\\\")[-1])\n",
    "        file = re.sub(r'\\\\', '/', file)\n",
    "        if compound_key in record:\n",
    "            dup[file] = record[compound_key]\n",
    "        else:\n",
    "            record[compound_key] = file\n",
    "        \n",
    "    return dup\n",
    "\n",
    "# if __name__ = '__main__':\n",
    "#     path = sys.argv[1]\n",
    "#     csv_path = sys.argv[2]\n",
    "#     if not os.path.isdir(path) or not os.path.isdir(csv_path) or csv_path[-1] != '\\\\':\n",
    "#         print('参数不是一个有效的文件夹！')\n",
    "#         while open(u'{csv_path}重复文件.csv'.format(csv_path=csv_path), 'w+') as csvfile:\n",
    "#             # 原文件 重复文件\n",
    "#             header = ['Source', 'Duplicate']\n",
    "#             writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "#             writer.writerheader()\n",
    "#             print(\"开始遍历文件夹，寻找重复文件，请等待....\")\n",
    "#             print(\"开始写入csv文件，请等待...\")\n",
    "#             for file in findDupes(path).items():\n",
    "#                 writer.writerow({\"Source\": file[1], \"Duplicate\": file[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33be803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDupes(path):\n",
    "    record = {}\n",
    "    dup = {}\n",
    "    d = diskwalk(path)\n",
    "    files = d.paths()\n",
    "    for file in files:\n",
    "        #compound_key = (getsize(file), create_checksum(file))\n",
    "        compound_key = (getsize(file), file.split(\"\\\\\")[-1])\n",
    "        file = re.sub(r'\\\\', '/', file)\n",
    "        if compound_key in record:\n",
    "            dup[file] = record[compound_key]\n",
    "        else:\n",
    "            record[compound_key] = file\n",
    "        \n",
    "    return dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "baafe8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'./duplicates.csv'\n",
    "path = r'E:\\程序代码备份'\n",
    "#findDupes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "97d44cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始遍历文件夹，寻找重复文件，请等待....\n",
      "开始写入csv文件，请等待...\n",
      "写入文件完成！\n"
     ]
    }
   ],
   "source": [
    "with open(f'{csv_path}.csv', 'w', encoding='utf8') as csvfile:\n",
    "    # 原文件 重复文件\n",
    "    header = ['Source', 'Duplicate']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    print(\"开始遍历文件夹，寻找重复文件，请等待....\")\n",
    "    print(\"开始写入csv文件，请等待...\")\n",
    "    for file in findDupes(path).items():\n",
    "        writer.writerow({\"Source\": file[1], \"Duplicate\": file[0]})\n",
    "        \n",
    "    print('写入文件完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895de47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
